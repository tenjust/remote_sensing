{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"D:/Yehmh/test_py/species/5m_5m\"\n",
    "\n",
    "X = []  # Features\n",
    "y = []  # Labels\n",
    "\n",
    "\n",
    "for folder in os.listdir(data_dir):\n",
    "    if os.path.isdir(os.path.join(data_dir, folder)):  # Check if it's a directory\n",
    "        species = folder  # Assuming folder name is the species label\n",
    "        species_dir = os.path.join(data_dir, folder)\n",
    "        for file in os.listdir(species_dir):\n",
    "            if file.endswith(\".tif\"):\n",
    "                image_path = os.path.join(species_dir, file)\n",
    "                image = cv2.imread(image_path)\n",
    "                image = cv2.resize(image, (224, 224))  # Resize image to 224x224\n",
    "                X.append(image)\n",
    "                y.append(species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bischofia_javanica' 'Fraxinus_griffithii' 'Pterocarpus_indicus'\n",
      " 'Swietenia_macrophylla' 'Terminalia_catappa']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4]\n",
      "329 83\n"
     ]
    }
   ],
   "source": [
    "print(label_encoder.classes_)\n",
    "print(y)\n",
    "print(len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define transformations and create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define transform for ResNet-50\n",
    "# transform_resnet = transforms.Compose([\n",
    "#     transforms.ToPILImage(),  # Convert to PIL Image\n",
    "#     transforms.Resize((224, 224)),  # Resize to 224x224\n",
    "#     transforms.ToTensor(),  # Convert image to PyTorch tensor\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize image data\n",
    "# ])\n",
    "\n",
    "# # Apply transform_resnet for ResNet-50\n",
    "# train_dataset = CustomDataset(X_train, y_train, transform=transform_resnet)\n",
    "# test_dataset = CustomDataset(X_test, y_test, transform=transform_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation ------\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # Convert to PIL Image\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the images horizontally\n",
    "    transforms.RandomRotation(degrees=15),  # Randomly rotate the images by up to 15 degrees\n",
    "    transforms.RandomCrop(size=(224, 224), padding=4),  # Randomly crop a 224x224 region of the image with padding\n",
    "    transforms.ToTensor(),  # Convert image to PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize image data\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # Convert to PIL Image\n",
    "    transforms.Resize((224, 224)),  # Resize image to 224x224\n",
    "    transforms.ToTensor(),  # Convert image to PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize image data\n",
    "])\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train, transform=train_transform)\n",
    "test_dataset = CustomDataset(X_test, y_test, transform=test_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "# test_loader is created with shuffle=False. \n",
    "# Ensures that the evaluation process remains consistent across different evaluations \n",
    "# and that the model is tested on the same data distribution every time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Define the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet50, self).__init__()\n",
    "        self.resnet = resnet50(pretrained=True)  # Load pretrained ResNet-50\n",
    "        # Replace the last fully connected layer with a new one\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Initialize model. loss function, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CIRES\\anaconda3\\envs\\test\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\CIRES\\anaconda3\\envs\\test\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = ResNet50(num_classes=len(label_encoder.classes_))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1.1404084671627392, Accuracy: 0.5987841945288754\n",
      "Epoch [2/100], Loss: 0.9857872074300592, Accuracy: 0.7355623100303952\n",
      "Epoch [3/100], Loss: 0.8378311043435877, Accuracy: 0.7416413373860182\n",
      "Epoch [4/100], Loss: 0.6737399670210752, Accuracy: 0.756838905775076\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     11\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     14\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()   \u001b[38;5;66;03m# Zero the gradient\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(images) \u001b[38;5;66;03m# Forward pass: compute predicted outputs\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\CIRES\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\CIRES\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\CIRES\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\CIRES\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[23], line 29\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     26\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 29\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "File \u001b[1;32mc:\\Users\\CIRES\\anaconda3\\envs\\test\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\CIRES\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\CIRES\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\CIRES\\anaconda3\\envs\\test\\lib\\site-packages\\torchvision\\transforms\\transforms.py:712\u001b[0m, in \u001b[0;36mRandomHorizontalFlip.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    705\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    707\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be flipped.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Randomly flipped image.\u001b[39;00m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 712\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m:\n\u001b[0;32m    713\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mhflip(img)\n\u001b[0;32m    714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()   # Zero the gradient\n",
    "        outputs = model(images) # Forward pass: compute predicted outputs\n",
    "        loss = criterion(outputs, labels) # Compute the loss\n",
    "        loss.backward()     # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        optimizer.step()    # Update the model parameters based on the gradients\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    train_accuracy = correct / total\n",
    "    train_loss = epoch_loss / len(train_loader)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss}, Accuracy: {train_accuracy}\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), train_accuracies, label='Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 3, Actual: 3\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 2, Actual: 2\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 4, Actual: 4\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 3\n",
      "Predicted: 2, Actual: 2\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 4, Actual: 4\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 4, Actual: 4\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 4, Actual: 4\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 2, Actual: 2\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 4, Actual: 4\n",
      "Predicted: 4, Actual: 4\n",
      "Predicted: 4, Actual: 4\n",
      "Predicted: 2, Actual: 2\n",
      "Predicted: 4, Actual: 4\n",
      "Predicted: 2, Actual: 2\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 4, Actual: 4\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 3, Actual: 2\n",
      "Predicted: 4, Actual: 4\n",
      "Predicted: 2, Actual: 2\n",
      "Predicted: 4, Actual: 4\n",
      "Predicted: 4, Actual: 4\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 4, Actual: 4\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 2, Actual: 2\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 4, Actual: 4\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 2, Actual: 2\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 3, Actual: 0\n",
      "Predicted: 2, Actual: 2\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 3, Actual: 0\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 2, Actual: 2\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 4, Actual: 4\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 1, Actual: 0\n",
      "Predicted: 3, Actual: 4\n",
      "Predicted: 3, Actual: 4\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 2, Actual: 2\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 4, Actual: 4\n",
      "Predicted: 2, Actual: 2\n",
      "Test Accuracy: 91.57%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Print the predicted and actual labels\n",
    "        for i in range(len(labels)):\n",
    "            print(\"Predicted: {}, Actual: {}\".format(predicted[i], labels[i]))\n",
    "\n",
    "accuracy = correct / total\n",
    "print('Test Accuracy: {:.2f}%'.format(100 * accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'resnet_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import torch\n",
    "# import rasterio\n",
    "# import numpy as np\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision import transforms\n",
    "# # from your_model_file import CNN  # Import your CNN model class\n",
    "# import csv\n",
    "\n",
    "# # Function to extract coordinates from TIF files\n",
    "# def extract_coordinates(tif_file):\n",
    "#     with rasterio.open(tif_file) as src:\n",
    "#         crs = src.crs\n",
    "#         bounds = src.bounds\n",
    "#         center_x = (bounds.left + bounds.right) / 2\n",
    "#         center_y = (bounds.top + bounds.bottom) / 2\n",
    "#         return crs, (center_x, center_y)\n",
    "\n",
    "# # Define directory containing unknown TIF files\n",
    "# # unknown_photos_dir = 'D:\\\\Yehmh\\\\test_py\\\\202301\\\\P00073_transect_234\\\\10m_10m\\\\unknown'\n",
    "\n",
    "# def classify_and_write(unknown_photos_dir, output_csv_path):\n",
    "\n",
    "#     # Initialize lists to store unknown photo paths, coordinates, and predictions\n",
    "#     unknown_photos = []\n",
    "#     unknown_coordinates = []\n",
    "\n",
    "#     # Iterate over unknown TIF files\n",
    "#     for filename in os.listdir(unknown_photos_dir):\n",
    "#         if filename.endswith('.tif'):\n",
    "#             tif_file = os.path.join(unknown_photos_dir, filename)\n",
    "            \n",
    "#             # Extract coordinates\n",
    "#             crs, coordinates = extract_coordinates(tif_file)\n",
    "            \n",
    "#             # Append to the list\n",
    "#             unknown_photos.append(tif_file)\n",
    "#             unknown_coordinates.append((filename, crs, coordinates))\n",
    "\n",
    "#     # Now you have a list of unknown photo paths (unknown_photos) and corresponding coordinates (unknown_coordinates)\n",
    "\n",
    "#     # Load the trained model\n",
    "#     # model = CNN(num_classes=len(label_encoder.classes_))\n",
    "#     # model.load_state_dict(torch.load('path/to/your/trained/model.pth'))\n",
    "#     model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "#     # Define transformations for the images\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.ToTensor(),  \n",
    "#         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  \n",
    "#     ])\n",
    "\n",
    "#     # Make predictions on the unknown photos\n",
    "#     predictions = []\n",
    "#     for photo_path in unknown_photos:\n",
    "#         # Load and preprocess the image\n",
    "#         image = cv2.imread(photo_path)\n",
    "#         image = cv2.resize(image, (64, 64))  # Resize image if necessary\n",
    "#         image = transform(image)\n",
    "#         image = image.unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "#         # Perform inference\n",
    "#         with torch.no_grad():\n",
    "#             output = model(image)\n",
    "#             predicted_class = torch.argmax(output).item()\n",
    "#             predicted_label = label_encoder.classes_[predicted_class]\n",
    "#             predictions.append(predicted_label)\n",
    "\n",
    "#     # Now you have the predictions for each unknown photo in the list 'predictions'\n",
    "#     # You can proceed to visualize the results on a map using the coordinates\n",
    "#     with open(output_csv_path, 'w', newline='') as csvfile:\n",
    "#         writer = csv.writer(csvfile)\n",
    "#         writer.writerow(['Filename', 'Latitude', 'Longitude', 'Predicted Species'])  # Write header\n",
    "#         for coord, label in zip(unknown_coordinates, predictions):\n",
    "#             filename, crs, (latitude, longitude) = coord\n",
    "#             writer.writerow([filename, latitude, longitude, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# # Define output CSV file path\n",
    "# # output_csv_path = 'D:/Yehmh/test_py/202301/P00073_transect_234/10m_10m/species_distribution.csv'\n",
    "\n",
    "\n",
    "# # Write results to CSV file\n",
    "# with open(output_csv_path, 'w', newline='') as csvfile:\n",
    "#     writer = csv.writer(csvfile)\n",
    "#     writer.writerow(['Filename', 'Latitude', 'Longitude', 'Predicted Species'])  # Write header\n",
    "#     for coord, label in zip(unknown_coordinates, predictions):\n",
    "#         filename, crs, (latitude, longitude) = coord\n",
    "#         writer.writerow([filename, latitude, longitude, label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list = [69, 70, 71, 75, 76, 78, 79, 82]\n",
    "\n",
    "# for i in list:\n",
    "#     unknown_photos_dir = f'D:\\\\Yehmh\\\\test_py\\\\202301\\\\P000{i}\\\\5m_5m'\n",
    "#     output_csv_path = f'D:/Yehmh/test_py/202301/P000{i}_species_distribution.csv'\n",
    "    \n",
    "#     classify_and_write(unknown_photos_dir, output_csv_path)\n",
    "    \n",
    "#     print(i, \" done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import torch\n",
    "# import rasterio\n",
    "# import numpy as np\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision import transforms\n",
    "# # from your_model_file import CNN  # Import your CNN model class\n",
    "# import csv\n",
    "\n",
    "# # Function to extract coordinates from TIF files\n",
    "# def extract_coordinates(tif_file):\n",
    "#     with rasterio.open(tif_file) as src:\n",
    "#         crs = src.crs\n",
    "#         bounds = src.bounds\n",
    "#         center_x = (bounds.left + bounds.right) / 2\n",
    "#         center_y = (bounds.top + bounds.bottom) / 2\n",
    "#         return crs, (center_x, center_y)\n",
    "\n",
    "# # Define directory containing unknown TIF files\n",
    "# # unknown_photos_dir = 'D:\\\\Yehmh\\\\test_py\\\\202301\\\\P00073_transect_234\\\\10m_10m\\\\unknown_cleaned'\n",
    "\n",
    "# def classify_and_write(unknown_photos_dir, output_csv_path):\n",
    "\n",
    "#     # Initialize lists to store unknown photo paths, coordinates, and predictions\n",
    "#     unknown_photos = []\n",
    "#     unknown_coordinates = []\n",
    "\n",
    "#     # Iterate over unknown TIF files\n",
    "#     for filename in os.listdir(unknown_photos_dir):\n",
    "#         if filename.endswith('.tif'):\n",
    "#             tif_file = os.path.join(unknown_photos_dir, filename)\n",
    "            \n",
    "#             # Extract coordinates\n",
    "#             crs, coordinates = extract_coordinates(tif_file)\n",
    "            \n",
    "#             # Append to the list\n",
    "#             unknown_photos.append(tif_file)\n",
    "#             unknown_coordinates.append((filename, crs, coordinates))\n",
    "\n",
    "#     # Now you have a list of unknown photo paths (unknown_photos) and corresponding coordinates (unknown_coordinates)\n",
    "\n",
    "#     # Define a function to classify with threshold\n",
    "#     def classify_with_threshold(probabilities, threshold):\n",
    "#         max_prob, max_index = torch.max(probabilities, dim=1)\n",
    "#         if max_prob.item() < threshold:\n",
    "#             return \"unknown\"\n",
    "#         else:\n",
    "#             predicted_label = label_encoder.classes_[max_index.item()]\n",
    "#             return predicted_label  # Return the index of the class with the maximum probability\n",
    "\n",
    "\n",
    "#     # Load the trained model\n",
    "#     # model = CNN(num_classes=len(label_encoder.classes_))\n",
    "#     # model.load_state_dict(torch.load('path/to/your/trained/model.pth'))\n",
    "#     model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "#     # Define transformations for the images\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.ToTensor(),  \n",
    "#         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  \n",
    "#     ])\n",
    "\n",
    "#     # Make predictions on the unknown photos\n",
    "#     predictions = []\n",
    "#     for photo_path in unknown_photos:\n",
    "#         # Load and preprocess the image\n",
    "#         image = cv2.imread(photo_path)\n",
    "#         image = cv2.resize(image, (64, 64))  # Resize image if necessary\n",
    "#         image = transform(image)\n",
    "#         image = image.unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "#         # Perform inference\n",
    "#         with torch.no_grad():\n",
    "#             output = model(image)\n",
    "#             probabilities = nn.functional.softmax(output, dim=1)  # Apply softmax to get probabilities\n",
    "#             classification = classify_with_threshold(probabilities, threshold=0.7)  # Adjust threshold as needed\n",
    "#             predictions.append(classification)\n",
    "\n",
    "#     # Now you have the predictions for each unknown photo in the list 'predictions'\n",
    "#     # You can proceed to visualize the results on a map using the coordinates\n",
    "#     with open(output_csv_path, 'w', newline='') as csvfile:\n",
    "#         writer = csv.writer(csvfile)\n",
    "#         writer.writerow(['Filename', 'Latitude', 'Longitude', 'Predicted Species'])  # Write header\n",
    "#         for coord, label in zip(unknown_coordinates, predictions):\n",
    "#             filename, crs, (latitude, longitude) = coord\n",
    "#             writer.writerow([filename, latitude, longitude, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# # Define output CSV file path\n",
    "# output_csv_path = 'D:/Yehmh/test_py/202301/P00073_transect_234/10m_10m/species_distribution_unclassified_2.csv'\n",
    "\n",
    "# # Write results to CSV file\n",
    "# with open(output_csv_path, 'w', newline='') as csvfile:\n",
    "#     writer = csv.writer(csvfile)\n",
    "#     writer.writerow(['Filename', 'Latitude', 'Longitude', 'Predicted Species'])  # Write header\n",
    "#     for coord, label in zip(unknown_coordinates, predictions):\n",
    "#         filename, crs, (latitude, longitude) = coord\n",
    "#         writer.writerow([filename, latitude, longitude, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list = [69, 70, 71, 75, 76, 78, 79, 82]\n",
    "# list = [76, 78, 79, 82]\n",
    "# list = [78]\n",
    "\n",
    "# for i in list:\n",
    "#     unknown_photos_dir = f'D:\\\\Yehmh\\\\test_py\\\\202301\\\\P000{i}\\\\5m_5m'\n",
    "#     output_csv_path = f'D:/Yehmh/test_py/202301/P000{i}_species_distribution_prob_2.csv'\n",
    "    \n",
    "#     classify_and_write(unknown_photos_dir, output_csv_path)\n",
    "    \n",
    "#     print(i, \" done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unknown_photos_dir = f'D:\\\\Yehmh\\\\test_py\\\\202301\\\\P00074_transect_1\\\\5m_5m'\n",
    "# output_csv_path = f'D:/Yehmh/test_py/202301/P00074_species_distribution_prob.csv'\n",
    "# classify_and_write(unknown_photos_dir, output_csv_path)\n",
    "# print(\"done 1\")\n",
    "\n",
    "# unknown_photos_dir = f'D:\\\\Yehmh\\\\test_py\\\\202301\\\\P00073_transect_234\\\\5m_5m\\\\unknown'\n",
    "# output_csv_path = f'D:/Yehmh/test_py/202301/P00073_species_distribution_prob.csv'\n",
    "# classify_and_write(unknown_photos_dir, output_csv_path)\n",
    "# print(\"done 2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
